@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{ren2015fasterrcnn,
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume={28},
  year={2015}
}

@inproceedings{girshick2015fast,
  title={Fast R-CNN},
  author={Girshick, Ross},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015},
  pages={1440--1448},
}

@inproceedings{he2017maskrcnn,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@inproceedings{redmon2016yolo,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@inproceedings{liu2016ssd,
  title={Ssd: Single shot multibox detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle={European conference on computer vision},
  pages={21--37},
  year={2016},
  organization={Springer}
}

@inproceedings{carion2020detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{
zhu2020deformabledetr,
title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},
author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=gZ9hCDWe6ke}
}

@inproceedings{li2022dndetr,
  title={Dn-detr: Accelerate detr training by introducing query denoising},
  author={Li, Feng and Zhang, Hao and Liu, Shilong and Guo, Jian and Ni, Lionel M and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13619--13627},
  year={2022}
}

@inproceedings{liu2021dabdetr,
  title={DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR},
  author={Liu, Shilong and Li, Feng and Zhang, Hao and Yang, Xiao and Qi, Xianbiao and Su, Hang and Zhu, Jun and Zhang, Lei},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{zhang2022dinodetr,
  title={Dino: Detr with improved denoising anchor boxes for end-to-end object detection},
  author={Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2203.03605},
  year={2022}
}

@inproceedings{lin2017focalloss,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@article{munkres1957algorithmstransportationhungarian,
  title={Algorithms for the assignment and transportation problems},
  author={Munkres, James},
  journal={Journal of the society for industrial and applied mathematics},
  volume={5},
  number={1},
  pages={32--38},
  year={1957},
  publisher={SIAM}
}

@article{liu2020objectdetectionsurvey,
  title={Deep learning for generic object detection: A survey},
  author={Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"a}inen, Matti},
  journal={International journal of computer vision},
  volume={128},
  number={2},
  pages={261--318},
  year={2020},
  publisher={Springer}
}

@inproceedings{girshick2014rcnn,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@article{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{knight2013fastbalancing,
  title={A fast algorithm for matrix balancing},
  author={Knight, Philip A and Ruiz, Daniel},
  journal={IMA Journal of Numerical Analysis},
  volume={33},
  number={3},
  pages={1029--1047},
  year={2013},
  publisher={Oxford University Press}
}

@article{frogner2015learningwasserstein,
  title={Learning with a Wasserstein loss},
  author={Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya, Mauricio and Poggio, Tomaso A},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{chizat2018unbalanced,
  title={Unbalanced optimal transport: Dynamic and Kantorovich formulations},
  author={Chizat, Lenaic and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c{c}}ois-Xavier},
  journal={Journal of Functional Analysis},
  volume={274},
  number={11},
  pages={3090--3123},
  year={2018},
  publisher={Elsevier}
}

@article{chizat2018scaling,
  title={Scaling algorithms for unbalanced optimal transport problems},
  author={Chizat, Lenaic and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c{c}}ois-Xavier},
  journal={Mathematics of Computation},
  volume={87},
  number={314},
  pages={2563--2609},
  year={2018}
}

@article{liero2018optimal,
  title={Optimal entropy-transport problems and a new Hellinger--Kantorovich distance between positive measures},
  author={Liero, Matthias and Mielke, Alexander and Savar{\'e}, Giuseppe},
  journal={Inventiones mathematicae},
  volume={211},
  number={3},
  pages={969--1117},
  year={2018},
  publisher={Springer}
}

@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@article{monge1781memoire,
  title={M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
  author={Monge, Gaspard},
  journal={Mem. Math. Phys. Acad. Royale Sci.},
  pages={666--704},
  year={1781}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{buckland1994precisionrecall,
  title={The relationship between recall and precision},
  author={Buckland, Michael and Gey, Fredric},
  journal={Journal of the American society for information science},
  volume={45},
  number={1},
  pages={12--19},
  year={1994},
  publisher={Wiley Online Library}
}

@inproceedings{fatras2021unbalanced,
  title={Unbalanced minibatch optimal transport; applications to domain adaptation},
  author={Fatras, Kilian and S{\'e}journ{\'e}, Thibault and Flamary, R{\'e}mi and Courty, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={3186--3197},
  year={2021},
  organization={PMLR}
}

@book{brualdi_2006,
place={Cambridge},
series={Encyclopedia of Mathematics and its Applications},
title={Combinatorial Matrix Classes},
DOI={10.1017/CBO9780511721182},
publisher={Cambridge University Press},
author={Brualdi, Richard A.},
year={2006},
collection={Encyclopedia of Mathematics and its Applications}
}

@INPROCEEDINGS{giou,
author = {H. Rezatofighi and N. Tsoi and J. Gwak and A. Sadeghian and I. Reid and S. Savarese},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression},
year = {2019},
volume = {},
issn = {},
pages = {658-666},
abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
keywords = {},
doi = {10.1109/CVPR.2019.00075},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00075},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@article{santambrogio,
  added-at = {2019-12-11T00:48:10.000+0100},
  author = {Santambrogio, Filippo},
  biburl = {https://www.bibsonomy.org/bibtex/281ed66090d576f173cc088fb4596d431/kirk86},
  description = {OTAM-cvgmt.pdf},
  interhash = {d50122af28034ea04f9891cf91bd5af4},
  intrahash = {81ed66090d576f173cc088fb4596d431},
  keywords = {book optimal-transport},
  timestamp = {2019-12-11T00:48:58.000+0100},
  title = {Optimal Transport for Applied Mathematicians. Calculus of Variations, PDEs and Modeling},
  url = {https://www.math.u-psud.fr/~filippo/OTAM-cvgmt.pdf},
  year = 2015
}

@article{kantorovich,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2626967},
 abstract = {The following paper is reproduced from a Russian journal of the character of our own Proceedings of the National Academy of Sciences, Comptes Rendus (Doklady) de l'Académie des Sciences de l'URSS, 1942, Volume XXXVII, No. 7-8. The author is one of the most distinguished of Russian mathematicians. He has made very important contributions in pure mathematics in the theory of functional analysis, and has made equally important contributions to applied mathematics in numerical analysis and the theory and practice of computation. Although his exposition in this paper is quite terse and couched in mathematical language which may be difficult for some readers of Management Science to follow, it is thought that this presentation will: (1) make available to American readers generally an important work in the field of linear programming, (2) provide an indication of the type of analytic work which has been done and is being done in connection with rational planning in Russia, (3) through the specific examples mentioned indicate the types of interpretation which the Russians have made of the abstract mathematics (for example, the potential and field interpretations adduced in this country recently by W. Prager were anticipated in this paper).},
 author = {L. Kantorovitch},
 journal = {Management Science},
 number = {1},
 pages = {1--4},
 publisher = {INFORMS},
 title = {On the Translocation of Masses},
 urldate = {2022-11-03},
 volume = {5},
 year = {1958}
}

@inproceedings{lightspeed,
 author = {Cuturi, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 url = {https://proceedings.neurips.cc/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{schmitzer2019stabilized,
  title={Stabilized sparse scaling algorithms for entropy regularized transport problems},
  author={Schmitzer, Bernhard},
  journal={SIAM Journal on Scientific Computing},
  volume={41},
  number={3},
  pages={A1443--A1481},
  year={2019},
  publisher={SIAM}
}

@inproceedings{greenkorn,
 author = {Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},
 url = {https://proceedings.neurips.cc/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{screenkorn,
  title={Screening sinkhorn algorithm for regularized optimal transport},
  author={Alaya, Mokhtar Z and Berar, Maxime and Gasso, Gilles and Rakotomamonjy, Alain},
  booktitle = {Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@InProceedings{sinkhorn-complexity,
  title = 	 {Sample Complexity of Sinkhorn Divergences},
  author =       {Genevay, Aude and Chizat, L\'{e}na\"{i}c and Bach, Francis and Cuturi, Marco and Peyr\'{e}, Gabriel},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1574--1583},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/genevay19a/genevay19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/genevay19a.html},
  abstract = 	 {Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures.  We focus in this paper on Sinkhorn divergences (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength $\varepsilon$, between OT ($\varepsilon=0$) and MMD ($\varepsilon=\infty$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively $O(n^3\log n)$, $O(n^2)$ and $n^2$ operations given a sample size $n$), much less is known in terms of their sample complexity, namely the gap between these quantities, when evaluated using finite samples vs. their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, $1/n^{1/d}$ for OT in dimension $d$ and $1/\sqrt{n}$ for MMD, that for SDs has only been studied empirically. In this paper, we (i) derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer $\varepsilon$, (ii) prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and (iii) provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in $1/\sqrt{n}$ (as in MMD), with a constant that depends however on $\varepsilon$, making the bridge between OT and MMD complete.}
}

@article{CHIZAT20183090,
title = {Unbalanced optimal transport: Dynamic and Kantorovich formulations},
journal = {Journal of Functional Analysis},
volume = {274},
number = {11},
pages = {3090-3123},
issn = {0022-1236},
doi = {https://doi.org/10.1016/j.jfa.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022123618301058},
author = {Lénaïc Chizat and Gabriel Peyré and Bernhard Schmitzer and François-Xavier Vialard},
keywords = {Unbalanced optimal transport},
abstract = {This article presents a new class of distances between arbitrary nonnegative Radon measures inspired by optimal transport. These distances are defined by two equivalent alternative formulations: (i) a dynamic formulation defining the distance as a geodesic distance over the space of measures (ii) a static “Kantorovich” formulation where the distance is the minimum of an optimization problem over pairs of couplings describing the transfer (transport, creation and destruction) of mass between two measures. Both formulations are convex optimization problems, and the ability to switch from one to the other depending on the targeted application is a crucial property of our models. Of particular interest is the Wasserstein–Fisher–Rao metric recently introduced independently by [7], [15]. Defined initially through a dynamic formulation, it belongs to this class of metrics and hence automatically benefits from a static Kantorovich formulation.}
}

@phdthesis{chizat-these,
  TITLE = {{Unbalanced Optimal Transport : Models, Numerical Methods, Applications}},
  AUTHOR = {Chizat, Lenaic},
  URL = {https://tel.archives-ouvertes.fr/tel-01881166},
  NUMBER = {2017PSLED063},
  SCHOOL = {{Universit{\'e} Paris sciences et lettres}},
  YEAR = {2017},
  MONTH = Nov,
  KEYWORDS = {Optimal transport ; Convex analysis ; Optimization ; Nonnegative measures ; Information geometry ; Sinkhorn's algorithm ; Image processing ; Gradient flows ; Weak convergence ; Tensor field processing ; Barycentres ; Relative entropy ; Geodesic metric space ; Transport optimal ; Analyse convexe ; Optimisation ; Mesures positives ; G{\'e}om{\'e}trie de l'information ; Algorithme de Sinkhorn ; Traitement d'image ; Flots de gradient ; Convergence faible ; Traitement de champs de tenseurs ; Barycentres ; Entropie relative ; Espace m{\'e}trique g{\'e}od{\'e}sique},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-01881166/file/These-Finale-CHIZAT.pdf},
  HAL_ID = {tel-01881166},
  HAL_VERSION = {v1},
}


@InProceedings{genevay,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Genevay, Aude and Peyre, Gabriel and Cuturi, Marco},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}

@phdthesis{genevay-these,
  TITLE = {{Entropy-Regularized Optimal Transport for Machine Learning}},
  AUTHOR = {Genevay, Aude},
  URL = {https://tel.archives-ouvertes.fr/tel-02319318},
  SCHOOL = {{PSL University}},
  YEAR = {2019},
  MONTH = Mar,
  KEYWORDS = {Optimal transport ; Machine learning ; Transport optimal ; Apprentissage statistique},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-02319318/file/these_aude.pdf},
  HAL_ID = {tel-02319318},
  HAL_VERSION = {v1},
}

@article{wass-gaussians,
title = {The Fréchet distance between multivariate normal distributions},
journal = {Journal of Multivariate Analysis},
volume = {12},
number = {3},
pages = {450-455},
year = {1982},
issn = {0047-259X},
doi = {https://doi.org/10.1016/0047-259X(82)90077-X},
url = {https://www.sciencedirect.com/science/article/pii/0047259X8290077X},
author = {D.C Dowson and B.V Landau},
keywords = {Fréchet distance, multivariate normal distributions, covariance matrices},
abstract = {The Fréchet distance between two multivariate normal distributions having means μX, μY and covariance matrices ΣX, ΣY is shown to be given by d2 = |μX − μY|2 + tr(ΣX + ΣY − 2(ΣXΣY)12). The quantity d0 given by d02 = tr(ΣX + ΣY − 2(ΣXΣY)12) is a natural metric on the space of real covariance matrices of given order.}
}

@InProceedings{rotated-gaussians,
  title = 	 {Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss},
  author =       {Yang, Xue and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng and Tian, Qi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11830--11841},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21l/yang21l.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21l.html},
  abstract = 	 {Boundary discontinuity and its inconsistency to the final detection metric have been the bottleneck for rotating detection regression loss design. In this paper, we propose a novel regression loss based on Gaussian Wasserstein distance as a fundamental approach to solve the problem. Specifically, the rotated bounding box is converted to a 2-D Gaussian distribution, which enables to approximate the indifferentiable rotational IoU induced loss by the Gaussian Wasserstein distance (GWD) which can be learned efficiently by gradient back-propagation. GWD can still be informative for learning even there is no overlapping between two rotating bounding boxes which is often the case for small object detection. Thanks to its three unique properties, GWD can also elegantly solve the boundary discontinuity and square-like problem regardless how the bounding box is defined. Experiments on five datasets using different detectors show the effectiveness of our approach, and codes are available at https://github.com/yangxue0827/RotationDetection.}
}

@article{mixture-wasserstein,
author = {Delon, Julie and Desolneux, Agn\`{e}s},
title = {A Wasserstein-Type Distance in the Space of Gaussian Mixture Models},
journal = {SIAM Journal on Imaging Sciences},
volume = {13},
number = {2},
pages = {936-970},
year = {2020},
doi = {10.1137/19M1301047},
URL = { https://doi.org/10.1137/19M1301047},
eprint = {https://doi.org/10.1137/19M1301047},
abstract = { In this paper we introduce a Wasserstein-type distance on the set of Gaussian mixture models. This distance is defined by restricting the set of possible coupling measures in the optimal transport problem to Gaussian mixture models. We derive a very simple discrete formulation for this distance, which makes it suitable for high dimensional problems. We also study the corresponding multi-marginal and barycenter formulations. We show some properties of this Wasserstein-type distance, and we illustrate its practical use with some examples in image processing.}
}

@article{hungarian-cubic,
author = {Edmonds, Jack and Karp, Richard M.},
title = {Theoretical Improvements in Algorithmic Efficiency for Network Flow Problems},
year = {1972},
issue_date = {April 1972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321694.321699},
doi = {10.1145/321694.321699},
journal = {J. ACM},
month = {apr},
pages = {248–264},
numpages = {17}
}

@inproceedings{sinkhorn-divergences,
author = {Chizat, L\'{e}na\"{\i}c and Roussillon, Pierre and L\'{e}ger, Flavien and Vialard, Fran\c{c}ois-Xavier and Peyr\'{e}, Gabriel},
title = {Faster Wasserstein Distance Estimation with the Sinkhorn Divergence},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The squared Wasserstein distance is a natural quantity to compare probability distributions in a non-parametric setting. This quantity is usually estimated with the plug-in estimator, defined via a discrete optimal transport problem which can be solved to ε-accuracy by adding an entropic regularization of order ε and using for instance Sinkhorn's algorithm. In this work, we propose instead to estimate it with the Sinkhorn divergence, which is also built on entropic regularization but includes debiasing terms. We show that, for smooth densities, this estimator has a comparable sample complexity but allows higher regularization levels, of order ε1/2, which leads to improved computational complexity bounds and a strong speedup in practice. Our theoretical analysis covers the case of both randomly sampled densities and deterministic discretizations on uniform grids. We also propose and analyze an estimator based on Richardson extrapolation of the Sinkhorn divergence which enjoys improved statistical and computational efficiency guarantees, under a condition on the regularity of the approximation error, which is in particular satisfied for Gaussian densities. We finally demonstrate the efficiency of the proposed estimators with numerical experiments.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {190},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{gpu-hungarian,
title = {GPU-accelerated Hungarian algorithms for the Linear Assignment Problem},
journal = {Parallel Computing},
volume = {57},
pages = {52-72},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2016.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S016781911630045X},
author = {Ketan Date and Rakesh Nagi},
keywords = {Linear assignment problem, Parallel algorithm, Graphics processing unit, CUDA},
abstract = {In this paper, we describe parallel versions of two different variants (classical and alternating tree) of the Hungarian algorithm for solving the Linear Assignment Problem (LAP). We have chosen Compute Unified Device Architecture (CUDA) enabled NVIDIA Graphics Processing Units (GPU) as the parallel programming architecture because of its ability to perform intense computations on arrays and matrices. The main contribution of this paper is an efficient parallelization of the augmenting path search phase of the Hungarian algorithm. Computational experiments on problems with up to 25 million variables reveal that the GPU-accelerated versions are extremely efficient in solving large problems, as compared to their CPU counterparts. Tremendous parallel speedups are achieved for problems with up to 400 million variables, which are solved within 13 seconds on average. We also tested multi-GPU versions of the two variants on up to 16 GPUs, which show decent scaling behavior for problems with up to 1.6 billion variables and dense cost matrix structure.}
}

@inproceedings{gpu-bipartite,
  title={Bipartite graph matching computation on GPU},
  author={Vasconcelos, Cristina Nader and Rosenhahn, Bodo},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={42--55},
  year={2009},
  organization={Springer}
}

@incollection{gpu-matching,
  title={A GPU algorithm for greedy graph matching},
  author={Fagginger Auer, Bas O and Bisseling, Rob H},
  booktitle={Facing the Multicore-Challenge II},
  pages={108--119},
  year={2012},
  publisher={Springer}
}

@inproceedings{pang2019libra,
  title={Libra r-cnn: Towards balanced learning for object detection},
  author={Pang, Jiangmiao and Chen, Kai and Shi, Jianping and Feng, Huajun and Ouyang, Wanli and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={821--830},
  year={2019}
}

@inproceedings{cai2018cascade,
  title={Cascade r-cnn: Delving into high quality object detection},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6154--6162},
  year={2018}
}

@article{cai2019cascade,
  title={Cascade R-CNN: high quality object detection and instance segmentation},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={5},
  pages={1483--1498},
  year={2019},
  publisher={IEEE}
}

@article{dai2016r,
  title={R-FCN: Object detection via region-based fully convolutional networks},
  author={Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2117--2125},
  year={2017}
}

@article{he2015spatial,
  title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={9},
  pages={1904--1916},
  year={2015},
  publisher={IEEE}
}

@inproceedings{de2020wasserstein,
  title={Wasserstein exponential kernels},
  author={De Plaen, Henri and Fanuel, Micha{\"e}l and Suykens, Johan AK},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}


@article{montavon2016wasserstein,
  title={Wasserstein training of restricted Boltzmann machines},
  author={Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Cuturi, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}


@inproceedings{tolstikhin2017wasserstein,
  title={Wasserstein Auto-Encoders},
  author={Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{kolouri2018sliced,
  title={Sliced Wasserstein auto-encoders},
  author={Kolouri, Soheil and Pope, Phillip E and Martin, Charles E and Rohde, Gustavo K},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{rubenstein2018latent,
  title={On the latent space of wasserstein auto-encoders},
  author={Rubenstein, Paul K and Schoelkopf, Bernhard and Tolstikhin, Ilya},
  journal={arXiv preprint arXiv:1802.03761},
  year={2018}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

 @InProceedings{Han_2020_CVPR_Workshops,
author = {Han, Yuzhuo and Liu, Xiaofeng and Sheng, Zhenfei and Ren, Yutao and Han, Xu and You, Jane and Liu, Risheng and Luo, Zhongxuan},
title = {Wasserstein Loss-Based Deep Object Detection},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}

@InProceedings{pmlr-v139-yang21l,
  title = 	 {Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss},
  author =       {Yang, Xue and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng and Tian, Qi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11830--11841},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21l/yang21l.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21l.html}
}

@inproceedings{kolouri2016sliced,
  title={Sliced Wasserstein kernels for probability distributions},
  author={Kolouri, Soheil and Zou, Yang and Rohde, Gustavo K},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5258--5267},
  year={2016}
}

@inproceedings{ge2021ota,
  title={Ota: Optimal transport assignment for object detection},
  author={Ge, Zheng and Liu, Songtao and Li, Zeming and Yoshie, Osamu and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={303--312},
  year={2021}
}

@article{wang2021normalized,
  title={A normalized Gaussian Wasserstein distance for tiny object detection},
  author={Wang, Jinwang and Xu, Chang and Yang, Wen and Yu, Lei},
  journal={arXiv preprint arXiv:2110.13389},
  year={2021}
}

@inproceedings{otani2022optimal,
  title={Optimal Correction Cost for Object Detection Evaluation},
  author={Otani, Mayu and Togashi, Riku and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"a}, Janne and Satoh, Shin'ichi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21107--21115},
  year={2022}
}

@article{chen2022group,
  title={Group DETR: Fast Training Convergence with Decoupled One-to-Many Label Assignment},
  author={Chen, Qiang and Chen, Xiaokang and Zeng, Gang and Wang, Jingdong},
  journal={arXiv preprint arXiv:2207.13085},
  year={2022}
}


@article{vo2022review,
  title={A review on anchor assignment and sampling heuristics in deep learning-based object detection},
  author={Vo, Xuan-Thuy and Jo, Kang-Hyun},
  journal={Neurocomputing},
  year={2022},
  publisher={Elsevier}
}

@article{ge2021yolox,
  title={Yolox: Exceeding yolo series in 2021},
  author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
  journal={arXiv preprint arXiv:2107.08430},
  year={2021}
}

@ARTICLE{9152115,  author={Lee, John and Bertrand, Nicholas P. and Rozell, Christopher J.},  journal={IEEE Transactions on Computational Imaging},   title={Unbalanced Optimal Transport Regularization for Imaging Problems},   year={2020},  volume={6},  number={},  pages={1219-1232},  doi={10.1109/TCI.2020.3012954}}

@inproceedings{loftr,
  title={LoFTR: Detector-free local feature matching with transformers},
  author={Sun, Jiaming and Shen, Zehong and Wang, Yuang and Bao, Hujun and Zhou, Xiaowei},
  booktitle={
  CVPR},
  pages={8922--8931},
  year={2021}
}
